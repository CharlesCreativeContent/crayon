# Generative UI

# Using OpenAI Structured Outputs
Lets say we want to create a Conversational agent for a personal finance that reads your bank statements and assists you in figuring out your spending trends.

### Defining the user stories
Defining all possible templates for responses isn't possible with a Generative application since
the responses are generated by the LLM.

A good way to start is by defining the user stories that you expect most of your users to go through
and then define the response templates for each of the user stories. Over time, as more users use the application,
you can collect more data and improve the response templates.

For this example, we can define the following user stories:
1. User wants to know how much they have spent in the last month.
2. User wants to see how much they have spent on groceries in the current year.
3. User wants to set a budget limit for their expenses.

For fallback i.e. when the user asks a question that doesn't fit any of the user stories, we can use a generic markdown renderer.

### Building templates for the user stories
Now that we have defined the user stories, we can start building the templates for each of the user stories.
1. `breakdown_expenses`: Breaks down the expenses for the user for a time period.
2. `trends_expenses`: Shows the trends in the user's expenses for a time period.

### Using Structured Outputs to let the LLM select the appropriate template
Now that we have defined the templates, we can use Structured Outputs to let the LLM select the appropriate template.
In the json_schema, we ask LLM to select the appropriate template based on the user's query and fallback
to a generic text response if the user's query doesn't fit any of the user stories.

```ts
export const OutputJsonSchema = {
  type: "object",
  properties: {
    response: {
      type: "array",
      items: {
        oneOf: [
          {
            type: "object",
            description: "fallback when none of the templates are relevant",
            properties: {
              name: { const: "text" },
              text: {
                type: "string",
                description: "text message to be displayed to the user",
              },
            },
            required: ["text"],
            additionalProperties: false,
          },
          ...templates.map((template) => ({
            type: "object",
            description: template.description,
            properties: {
              name: { const: template.name },
              templateProps: template.parameters,
            },
            required: ["name", "parameters"],
            additionalProperties: false,
          })),
        ],
      },
    },
  },
} as const;
```

### Building the UI
By passing this `OutputJsonSchema` to the LLM and converting the LLM response to the [Streaming Protocol](/docs/concepts/data-format),
we can build stream the response back to the UI.

`<CrayonChat />` component handles the streaming protocol and state management rendering the response to the UI.
